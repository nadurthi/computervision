<!DOCTYPE HTML>
<html>
 <head>
  <meta charset="utf-8"/>
  <title>
   Made with Remarkable!
  </title>
  <link href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css" rel="stylesheet"/>
  <style type="text/css">
   body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}
  </style>
 </head>
 <body>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-markdown/2.10.0/js/bootstrap-markdown.js" type="text/javascript">
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>
  <div style="display: none;">
   $$
\(
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\NN}[3]{{\bf \mathcal{N}}(#1:#2,#3)}
\newcommand{\re}{\mathbb R}
\newcommand{\II}{{\bf \mathbb I}}
\newcommand{\bx}{{\bf x}}
\newcommand{\bmx}{{\bf \hat{x}}}
\newcommand{\bmxt}[2]{{\bf \hat{x}}_{#1}^{#2}}
\newcommand{\sigptset}{{\bf \mathbb (X;w)}}
\newcommand{\sigptsetele}{\mathbb (X^i;w^i)}
\newcommand{\xk}{\mathbf{x}_{k}}
\newcommand{\hhxk}{\mathbf{\hat{x}}_{k}}
\newcommand{\hhxkngk}{\mathbf{\hat{x}}_{k+1/k}}
\newcommand{\hhxkgk}{\mathbf{\hat{x}}_{k/k}}
\newcommand{\hhxkngkn}{\mathbf{\hat{x}}_{k+1/k+1}}
\newcommand{\hhxkn}{\mathbf{h}(\mathbf{x}_{k+1})}
\newcommand{\xkn}{\mathbf{x}_{k+1}}
\newcommand{\fxk}{\mathbf{f}(\mathbf{x}_{k})}
\newcommand{\hxkn}{\mathbf{h}(\mathbf{x}_{k+1})}
\newcommand{\ykn}{\mathbf{y}_{k+1}}
\newcommand{\hhykn}{\mathbf{\hat{y}}_{k+1}}
\newcommand{\Yk}{\mathbf{Y}_{k}}
\newcommand{\Ykn}{\mathbf{Y}_{k+1}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\eps}{\boldsymbol{\epsilon}}
\newcommand{\Nu}{\boldsymbol{\nu}}
\newcommand{\Ps}{\boldsymbol{\Psi}}
\newcommand{\xii}{\boldsymbol{\xi}}
\newcommand{\Al}{\boldsymbol{\alpha}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\hc}{\mathbf{H}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\Y}{\mathbf{Y}}
\)
$$
  </div>
  <h1 id="computer-vision-basics">
   Computer vision basics
  </h1>
  <p>
   <mathjax>
    $$x=a^2+b^2 $$
   </mathjax>
  </p>
  <h1 id="computer-vision-basics_1">
   Computer vision basics
  </h1>
  <p>
   \begin{align}
   <br/>
   x=a^2+1
   <br/>
   \end{align}
  </p>
  <h1 id="computer-vision-basics_2">
   Computer vision basics
  </h1>
  <p>
   <mathjax>
    $$
\begin{bmatrix}
1 &amp; 2\\
3 &amp; 4
\end{bmatrix}
$$
   </mathjax>
  </p>
  <h2 id="ok">
   OK
  </h2>
  <pre><code class="c++">#include &lt;iostream&gt;

int main(){

}
</code></pre>
  <pre><code class="python">def func1():
    a=a+a
</code></pre>
  <h1 id="kalman-filter">
   Kalman Filter
  </h1>
  <ol>
   <li>
    <p>
     Initial conditions, Process noise and Measurement noise are
     <font color="red">
      Gaussian
     </font>
     distributed and mutually independent processes.
    </p>
   </li>
   <li>
    <p>
     State and Measurement model equations are \alert{linear}.
    </p>
   </li>
  </ol>
  <p>
   <mathjax>
    $$
\begin{align}
\mbf{x}_{k+1}&amp;=F_k{x}_k+\nu_k \\
y_{k+1}&amp;=H_{k+1}{x}_{k+1}+\omega_{k+1}
\end{align}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align}
\mbf{x}_0 &amp;\sim \NN{\mbf{x}_0}{\mbf{\hat{x}}_0}{P_0}\\
\mbf{\nu}_k &amp;\sim \NN{\mbf{\nu}_k}{\mbf{0}}{Q_k}\\
\mbf{\omega}_{k+1} &amp;\sim \NN{\mbf{\omega}_{k+1}}{0}{R_{k+1}}
\end{align}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align}
p(\mbf{x}_k|Y_k)=\NN{\mbf{x}_k}{\mbf{\hat{x}}_{k|k}}{P_{k|k}}
\end{align}
$$
   </mathjax>
  </p>
  <p>
   where (Y_k) is set of measurement upto time (k),
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
p(\mbf{x}_{k+1}|Y_k)=\int p(\mbf{x}_{k+1}|\mbf{x}_{k})\: . \:p(\mbf{x}_{k}|Y_k)\:d\mbf{x}_{k}
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   The state transition probability density (p(\mbf{x}
   <em k="k">
    {k+1}|\mbf{x}
   </em>
   )) is given as
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
p(\mbf{x}_{k+1}|\mbf{x}_{k})=\NN{\mbf{x}_{k+1}}{F_k\mbf{{x}}_{k}}{Q_k}
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
p(\mbf{x}_{k+1}|Y_k)=\int \NN{\mbf{x}_{k+1}}{F_k\mbf{{x}}_{k}}{Q_k}\: . \: \NN{\mbf{x}_k}{\mbf{\hat{x}}_{k|k}}{P_{k|k}} \:d\mbf{x}_{k}
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
p(\mbf{x}_{k+1}|Y_k)&amp;=\frac{1}{\sqrt{(2\pi)^n |P_{k+1|k}|}} \: .\: \exp\Big[-\frac{1}{2}\big(\mbf{x}_{k+1}-\mbf{\hat{x}}_{k+1|k}\big)^T\:\:P_{k+1|k}^{-1}\:\:\big(\mbf{x}_{k+1}\mbf{\hat{x}}_{k+1|k} \big)\Big]
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   The prior pdf (p(\mbf{x}
   <em k_1_k="k+1|k">
    {k+1}|Y_k)) has mean (\mbf{\hat{x}}
   </em>
   ) and covariance (P_{k+1|k}) given as
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
{\mbf{\hat{x}}_{k+1|k}}&amp;={F_k\mbf{\hat{x}}_{k|k}}\\
{P_{k+1|k}}&amp;={F_kP_{k|k}F^T+Q_k}
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   As we know the pdf remains Gaussian, an easier approach is to directly compute the mean and covariance from the linear model equations as:
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
\mbf{\hat{x}}_{k+1|k}&amp;=E[\mbf{x}_{k+1}]=E\big[F_kx_k\big]+E[\nu_k]=F_kE\big[x_{k} \big]=F_k\mbf{\hat{x}}_{k|k}\\
P_{k+1|k}&amp;=E\big[ \big(\mbf{x}_{k+1}-F_k\mbf{\hat{x}}_{k|k} \big) \big(\mbf{x}_{k+1}-F_k\mbf{\hat{x}}_{k|k} \big)^T \big]=E\big[ \big(F_kx_k+\nu_k-F_k\mbf{\hat{x}}_{k|k} \big) \big(F_kx_k+\nu_k-F_k\mbf{\hat{x}}_{k|k} \big)^T \big]\\
&amp;=E\big[ F_k(x_k-\mbf{\hat{x}}_{k|k})(x_k-\mbf{\hat{x}}_{k|k})^TF_k^T+\nu_k\nu_k^T \big]=F_kP_{k|k}F^T+Q_k
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   Starting with the prior pdf at time step (k+1),
   <br/>
   <mathjax>
    $$
\begin{align*}
p(\mbf{x}_{k+1}|Y_k)=\NN{\mbf{x}_{k+1}}{\mbf{\hat{x}}_{k+1|k}\:\:}{P_{k+1|k}}
\end{align*}
$$
   </mathjax>
   <br/>
   the posterior pdf from Bayes’ rule at the same time step is given by
   <br/>
   <mathjax>
    $$
\begin{align*}
p(\mbf{x}_{k+1}|Y_{k+1})=\frac{p(\mbf{y}_{k+1}|\mbf{x}_{k+1})p(\mbf{x}_{k+1}|Y_k)}{\int p(\mbf{y}_{k+1}|\mbf{x}_{k+1})p(\mbf{x}_{k+1}|Y_k)\:d\mbf{x}_{k+1}}
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   where the measurement likelihood pdf (p(\mbf{y}
   <em k_1="k+1">
    {k+1}|\mbf{x}
   </em>
   )) can be derived from the measurement model equations as:
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
p(\mbf{y}_{k+1}|\mbf{x}_{k+1})=\NN{\mbf{y}_{k+1}}{H_{k+1}\mbf{x}_{k+1}}{\:\:R_{k+1}}
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   Numerator of Bayes’ rule is simplified as:
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
p(\mbf{y}_{k+1}|\mbf{x}_{k+1})p(\mbf{x}_{k+1}|Y_k)&amp;=\NN{\mbf{y}_{k+1}}{H_{k+1}\mbf{x}_{k+1}}{\:\:R_{k+1}}\NN{\mbf{x}_{k+1}}{\mbf{\hat{x}}_{k+1|k}\:\:}{P_{k+1|k}}
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
&amp;=\frac{1}{\sqrt{|2\pi R_{k+1}|}}\:.\: \frac{1}{\sqrt{|2\pi P_{k+1|k}|}}\: .\: \exp\Big[-\frac{1}{2}\big( \mbf{y}_{k+1}-H_{k+1}\mbf{x}_{k+1}\big)^TR_{k+1}^{-1}\big( \mbf{y}_{k+1}-H_{k+1}\mbf{x}_{k+1}\big) \\
&amp;\hspace{5cm} \: \: -\frac{1}{2}\big( \mbf{x}_{k+1}-\mbf{\hat{x}}_{k+1|k}\big)^T P_{k+1|k}^{-1} \big(\mbf{x}_{k+1}-\mbf{\hat{x}}_{k+1|k}\big) \Big]
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   The exponent is:
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
&amp;\Rightarrow-\frac{1}{2}\big( \mbf{y}_{k+1}-H_{k+1}\mbf{x}_{k+1}\big)^TR_{k+1}^{-1}\big( \mbf{y}_{k+1}-H_{k+1}\mbf{x}_{k+1}\big) -\frac{1}{2}\big( \mbf{x}_{k+1}-\mbf{\hat{x}}_{k+1|k}\big)^T P_{k+1|k}^{-1} \big(\mbf{x}_{k+1}-\mbf{\hat{x}}_{k+1|k}\big)\\
&amp;\Rightarrow -\frac{1}{2}\Big[ \mbf{y}_{k+1}^TR_{k+1}^{-1}\mbf{y}_{k+1} + \mbf{x}_{k+1}^T\underbrace{\big( P_{k+1|k}^{-1}+H_{k+1}^T R_{k+1}^{-1}H_{k+1}\big)}_{A}\mbf{x}_{k+1}\\
&amp;\hspace{4cm} -2 \underbrace{\big(\mbf{y}_{k+1}^TR_{k+1}^{-1}H_{k+1} +\mbf{\hat{x}}_{k+1|k}^T P_{k+1|k}^{-1}\big)}_{b^T}\mbf{x}_{k+1}+ \mbf{\hat{x}}_{k+1|k}^T P_{k+1|k}^{-1} \mbf{\hat{x}}_{k+1|k} \Big]\\
&amp;\Rightarrow -\frac{1}{2}\Big[ \mbf{y}_{k+1}^TR_{k+1}^{-1}\mbf{y}_{k+1} + \mbf{x}_{k+1}^TA\mbf{x}_{k+1} -2 b^T\mbf{x}_{k+1}+ \mbf{\hat{x}}_{k+1|k}^T P_{k+1|k}^{-1} \mbf{\hat{x}}_{k+1|k} \Big]\\
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   The denominator of Bayes’ rule is given as:
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
&amp;\int p(\mbf{y}_{k+1}|\mbf{x}_{k+1})p(\mbf{x}_{k+1}|Y_k)\:d\mbf{x}_{k+1}=\frac{1}{\sqrt{|2\pi R_{k+1}|}}\:.\: \frac{1}{\sqrt{|2\pi P_{k+1|k}|}}\: .\:\\
&amp;\int \exp\Big[ -\frac{1}{2} \mbf{y}_{k+1}^TR_{k+1}^{-1}\mbf{y}_{k+1} -\frac{1}{2} \mbf{x}_{k+1}^TA\mbf{x}_{k+1} +b^T\mbf{x}_{k+1}-\frac{1}{2} \mbf{\hat{x}}_{k+1|k}^T P_{k+1|k}^{-1} \mbf{\hat{x}}_{k+1|k} \Big]\:d\mbf{x}_{k+1}\\
&amp;=\frac{1}{\sqrt{|2\pi R_{k+1}|}}\:.\: \frac{1}{\sqrt{|2\pi P_{k+1|k}|}}\: .\: \exp\Big[ -\frac{1}{2} \mbf{y}_{k+1}^TR_{k+1}^{-1}\mbf{y}_{k+1}-\frac{1}{2} \mbf{\hat{x}}_{k+1|k}^T P_{k+1|k}^{-1} \mbf{\hat{x}}_{k+1|k} \Big]\\
&amp;\hspace{3cm}\: .\: \int \exp\Big[ -\frac{1}{2} \mbf{x}_{k+1}^TA\mbf{x}_{k+1} +b^T\mbf{x}_{k+1}\Big]\:d\mbf{x}_{k+1}\\
&amp;=\frac{\sqrt{|2\pi A^{-1}|}}{\sqrt{ |2\pi R_{k+1}||2\pi P_{k+1|k}|}}\: .\: \exp\Big[ -\frac{1}{2} \mbf{y}_{k+1}^TR_{k+1}^{-1}\mbf{y}_{k+1}-\frac{1}{2} \mbf{\hat{x}}_{k+1|k}^T P_{k+1|k}^{-1} \mbf{\hat{x}}_{k+1|k} +\frac{1}{2}b^TA^{-T}b \Big]
\end{align*}
$$
   </mathjax>
   <br/>
   The posterior pdf from Bayes’ rule is given as:
   <br/>
   <mathjax>
    $$
\begin{align*}
p(\mbf{x}_{k+1}|Y_k)&amp;=\frac{1}{\sqrt{|2\pi A^{-1}|}} \: . \: \exp \Big[-\frac{1}{2}\mbf{x}_{k+1}^TA\mbf{x}_{k+1} + b^T\mbf{x}_{k+1} -\frac{1}{2}b^TA^{-T}b \Big]\\
&amp;=\frac{1}{\sqrt{|2\pi A^{-1}|}} \: . \: \exp \Big[-\frac{1}{2}\big(\mbf{x}_{k+1}-A^{-1}b\big)^TA\big(\mbf{x}_{k+1}-A^{-1}b\big) \Big]\\
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
p(\mbf{x}_{k+1}|Y_{k+1})&amp;=\frac{1}{\sqrt{|2\pi A^{-1}|}} \: . \: \exp \Big[-\frac{1}{2}\big(\mbf{x}_{k+1}-A^{-1}b\big)^TA\big(\mbf{x}_{k+1}-A^{-1}b\big) \Big]
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
b^T&amp;=\big(\mbf{y}_{k+1}^TR_{k+1}^{-1}H_{k+1} +\mbf{\hat{x}}_{k+1|k}^T P_{k+1|k}^{-1}\big)\\
A&amp;= P_{k+1|k}^{-1}+H_{k+1}^T R_{k+1}^{-1}H_{k+1}\\
A^{-1}&amp;=P_{k+1|k}-\underbrace{P_{k+1|k}H_{k+1}^T\big(R_{k+1} +H_{k+1} P_{k+1|k} H_{k+1}^T \big)^{-1}}_{K_{k+1}} H_{k+1}P_{k+1|k}\\
&amp;=P_{k+1|k}-K_{k+1}H_{k+1}P_{k+1|k}\\
A^{-1}b&amp;=\big( P_{k+1|k}-K_{k+1}H_{k+1}P_{k+1|k} \big)\big(H_{k+1}^TR_{k+1}^{-1}\mbf{y}_{k+1} +P_{k+1|k}^{-1}\mbf{\hat{x}}_{k+1|k}\big)\\
&amp;=\mbf{\hat{x}}_{k+1|k}+K_{k+1}\big(\mbf{y}_{k+1}-H_{k+1}\mbf{\hat{x}}_{k+1|k} \big)
\end{align*}
$$
   </mathjax>
   <br/>
   Hence the posterior pdf still remains Gaussian even after Bayes’ Rule update, with mean and covariance as
   <br/>
   <mathjax>
    $$
\begin{align*}
\mbf{\hat{x}}_{k+1|k+1}&amp;=A^{-1}b=\mbf{\hat{x}}_{k+1|k}+K_{k+1}\big(\mbf{y}_{k+1}-H_{k+1}\mbf{\hat{x}}_{k+1|k} \big)\\
P_{k+1|k+1}&amp;=A^{-1}=P_{k+1|k}-K_{k+1}H_{k+1}P_{k+1|k}
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
p(\mbf{x}_{k+1}|Y_k)&amp;=\NN{\mbf{x}_{k+1}}{\mbf{\hat{x}}_{k+1|k}\:\:}{P_{k+1|k}}\\
\mbf{\hat{x}}_{k+1|k}&amp;=F_k\mbf{\hat{x}}_{k|k}\\
P_{k+1|k}&amp;=F_kP_{k|k}F^T+Q_k
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
p(\mbf{x}_{k+1}|Y_{k+1})&amp;=\NN{\mbf{x}_{k+1}}{\mbf{\hat{x}}_{k+1|k+1}\:\:}{P_{k+1|k+1}}\\
\mbf{\hat{x}}_{k+1|k+1}&amp;=\mbf{\hat{x}}_{k+1|k}+K_{k+1}\big(\mbf{y}_{k+1}-H_{k+1}\mbf{\hat{x}}_{k+1|k} \big)\\
P_{k+1|k+1}&amp;=P_{k+1|k}-K_{k+1}H_{k+1}P_{k+1|k}\\
K_{k+1}&amp;=P_{k+1|k}H_{k+1}^T\big(R_{k+1} +H_{k+1} P_{k+1|k} H_{k+1}^T \big)^{-1}
\end{align*}
$$
   </mathjax>
  </p>
  <h2>
   Minimum Variance Estimate
  </h2>
  <p>
   Starting from the prior pdf
   <mathjax>
    $p(\mbf{x}_{k+1}|Y_{k+1})=\NN{\mbf{x}_{k+1}}{\mbf{\hat{x}}_{k+1|k+1}\:\:}{P_{k+1|k+1}}$
   </mathjax>
   at time
   <mathjax>
    $k+1$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align}
\min\limits_{\hhxkngkn} \: Tr\Big\{E[(\xkn-\hhxkngkn)(\xkn-\hhxkngkn)^T]\Big\}
\end{align}
$$
   </mathjax>
  </p>
  <p>
   i.e. find an estimate that minimizes the posterior variance
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
\hhxkngkn =\hhxkngk+\mbf{K}_{k+1}(\ykn-\hhykn)
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   The assumed estimator is unbiased:
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
E[\hhxkngkn]&amp;=E[\hhxkngk+\mbf{K}_{k+1}(\ykn-\hhykn)]=E[\hhxkngk]+\mbf{K}_{k+1}(E[\ykn]-E[\hhykn])\\
&amp;=F_kE[\mbf{\hat{x}}_{k|k}]+\mbf{K}_{k+1}(H_{k+1}E[\mbf{x}_{k+1}]-H_{k+1}E[\hhxkngk])\\
&amp;=E[\mbf{x}_{k+1}]+\mbf{K}_{k+1}(H_{k+1}F\mbf{\hat{x}}_{k|k}-H_{k+1}F\mbf{\hat{x}}_{k|k})=E[\mbf{x}_{k+1}]
\end{align*}
$$
   </mathjax>
  </p>
  <h2>
   Variance approach for Kalman Filter
  </h2>
  <p>
   <mathjax>
    $$
\begin{align*}
&amp;\min\limits_{\mbf{K}_{k+1}} \: Tr\Big\{E[(\xkn-\hhxkngkn)(\xkn-\hhxkngkn)^T]\Big\}\\
&amp;\textit{with } \quad \hhxkngkn =\hhxkngk+\mbf{K}_{k+1}(\ykn-\hhykn)
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
&amp;\min\limits_{\mbf{K}_{k+1}} \: Tr\Big\{E[(\xkn-\hhxkngk-\mbf{K}_{k+1}(\ykn-\hhykn))(\xkn-\hhxkngk-\mbf{K}_{k+1}(\ykn-\hhykn))^T]\Big\}\\
&amp;\min\limits_{\mbf{K}_{k+1}} \: Tr\Big\{ E[\big(\xkn-\hhxkngk \big)\big(\xkn-\hhxkngk \big)^T] +\mbf{K}_{k+1}E[(\ykn-\hhykn)(\ykn-\hhykn)^T]\mbf{K}_{k+1}^T \\
&amp;\hspace{1cm} -\mbf{K}_{k+1}E[(\ykn-\hhykn)(\xkn-\hhxkngk)^T]- E[\big(\xkn-\hhxkngk \big)(\ykn-\hhykn)^T]\mbf{K}_{k+1}^T \Big\}\\
&amp;\min\limits_{\mbf{K}_{k+1}} \: Tr\Big\{P_{k+1|k}+ \mbf{K}_{k+1} P^y_{k+1|k} \mbf{K}_{k+1}^T-\mbf{K}_{k+1}P^{yx}_{k+1|k}-P^{xy}_{k+1|k} \mbf{K}_{k+1}^T \Big\}
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   The optimal gain
   <mathjax>
    $K_{k+1}$
   </mathjax>
   is given by
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
\mbf{K}_{k+1}&amp;=\mbf{P}^{xy}_{k+1}(\mbf{P}^{y}_{k+1/k})^{-1}
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
\hhykn &amp;=E[\mbf{y}_{k+1}]=E[H_{k+1}x_{k+1}+\omega_{k+1}]=H_{k+1}E[x_{k+1}]=H_{k+1}\mbf{\hat{x}}_{k+1|k}\\
%&amp;=H_{k+1}\int \mbf{x}_{k+1}\:\: \NN{\mbf{x}_{k+1}}{\mbf{\hat{x}}_{k+1|k}\:\:}{P_{k+1|k}}\:d\mbf{x}_{k+1}=H_{k+1}\mbf{\hat{x}}_{k+1|k}\\
{P}^{y}_{k+1/k}&amp;=E[(\ykn-\hhykn)(\ykn-\hhykn)^T]\\
&amp;=E[\big(H_{k+1}\mbf{x}_{k+1}+\omega_{k+1}-H_{k+1}\mbf{\hat{x}}_{k+1|k}\big)\big(H_{k+1}\mbf{x}_{k+1}+\omega_{k+1}-H_{k+1}\mbf{\hat{x}}_{k+1|k}\big)^T]\\
&amp;=H_{k+1}E[\big(\mbf{x}_{k+1}-\mbf{\hat{x}}_{k+1|k} \big)\big(\mbf{x}_{k+1}-\mbf{\hat{x}}_{k+1|k} \big)^T]H_{k+1}^T+E[\omega_{k+1}\omega_{k+1}^T]\\
&amp;=H_{k+1}P_{k+1|k}H_{k+1}^T+R_{k+1}\\
{P}^{xy}_{k+1}&amp;=E[\big(\xkn-\hhxkngk \big)(\ykn-\hhykn)^T]\\
&amp;=E[\big(\xkn-\hhxkngk \big) \big( H_{k+1}(\xkn-\hhxkngk)+\omega_{k+1} \big)^T]\\
&amp;=E[\big(\mbf{x}_{k+1}-\mbf{\hat{x}}_{k+1|k} \big)\big(\mbf{x}_{k+1}-\mbf{\hat{x}}_{k+1|k} \big)^T]H_{k+1}=P_{k+1|k}H_{k+1}^T
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   The Kalman Filter gain is then given as:
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
\mbf{K}_{k+1}&amp;=\mbf{P}^{xy}_{k+1}(\mbf{P}^{y}_{k+1/k})^{-1}=P_{k+1|k}H_{k+1}^T\big( H_{k+1}P_{k+1|k}H_{k+1}^T+R_{k+1}\big)^{-1}
\end{align*}
$$
   </mathjax>
  </p>
  <h2>
   Optimization approach - Recursive Least Squares
  </h2>
  <p>
   <mathjax>
    $$
\begin{align*}
J=\frac{1}{2}(\mbf{x}_0-\hat{\mbf{x}}_0)^TP_0^{-1}(\mbf{x}_0-\hat{\mbf{x}}_0)+\frac{1}{2}\sum_{k=1}^{T}(\mbf{y}_k-H_k\mbf{x}_k)^TR_k^{-1}(\mbf{y}_k-H_k\mbf{x}_k)
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   subject to:
  </p>
  <p>
   <mathjax>
    $$
\begin{align*}
\mbf{x}_{k+1}=F_k\mbf{x}_k
\end{align*}
$$
   </mathjax>
  </p>
  <p>
   minimize w.r.t.
   <mathjax>
    $[\mbf{x}_0,\mbf{x}_1,\cdots,\mbf{x}_T]$
   </mathjax>
  </p>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad();
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>
  <script type="text/javascript">
   MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});
  </script>
 </body>
</html>